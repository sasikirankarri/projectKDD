# -*- coding: utf-8 -*-
"""Copy of knowledge_graph_wiki_infoext_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g_mm5ULMsvl72_HV3SmyelRWiqGJSqj6
"""

! bash bootstrap.sh

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]

"""### Building Knowledge Graph
This application is about organizing information and making it easy to access by humans and computers alike.

### Implement the solution

Import the libraries
"""

import json
import re
import pandas as pd
import sparknlp

from pyspark.ml import Pipeline
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import lit, col,udf,explode , split

import sparknlp
from sparknlp import DocumentAssembler, Finisher
from sparknlp.annotator import *
from pyspark.sql.types import MapType, StringType, IntegerType, ArrayType 
import pandas as pd
import time
from neo4j import GraphDatabase, basic_auth
import time
from tqdm import tqdm
from urllib.request import urlopen
import urllib.request
from datetime import datetime

from utils.Neo4jConnection import Neo4jConnection
from utils.preprocess import cleanInfoBox,getPageIndexinCats,extractMovieEntity,extractPersonRelation

"""Start spark session"""

packages = [
    "com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4",
    'com.databricks:spark-xml_2.12:0.9.0'
]

spark = SparkSession.builder \
    .master("local[*]") \
    .appName("Knowledge Graph") \
    .config("spark.driver.memory", "12g") \
    .config("spark.jars.packages", ','.join(packages)) \
    .getOrCreate()

# spark.sparkContext.getConf().getAll()

"""##### Read the data from the wikipedia"""

data_start_time = time.time()
print("{} - started the data load into main dataframe".format(datetime.now()))

df = spark.read\
    .format('xml')\
    .option("rootTag", "mediawiki")\
    .option("rowTag", "page")\
    .load("*.bz2")\
    .persist()

getInfoboxUDF = udf(lambda x:cleanInfoBox(x))

"""Get the page indices from the categories of interest """

index_start_time = time.time()
print("{} - started the index extraction".format(datetime.now()))

"""Specify the category and wiki flag 

Please use the category for which you can find the data in wikipedia either simple wikipedia or english. By default , we have set this to simple to reduce the data

Allowed values for WIKI_FLAG is 'simple' or 'en'
"""

cat_config_list = []
cat_config_file = open('category.config','r')
l = cat_config_file.readlines()
for ele in l:
  res = ele.replace('"','').replace('\n',"").split(",")
  cat_config_list.append(res)

cat_config_list

pageIndexdf = spark.createDataFrame(getPageIndexinCats( cat_config_list,"N", "",[]), StringType()).withColumnRenamed("value","PageIndex")

pageContentdf = pageIndexdf.join(df,pageIndexdf.PageIndex == df.id,"inner").select('PageIndex','revision.text._VALUE')

print("Total pages: {}".format(pageContentdf.count()))

pageidDF = pageContentdf.filter(pageContentdf._VALUE.contains('{{Infobox')).select('PageIndex')
pageidlist = [int(row.PageIndex) for row in pageidDF.collect()]

pageidDF = pageContentdf.filter(pageContentdf._VALUE.contains('{{Infobox')).select('PageIndex').distinct()
pageidlist = [int(row.PageIndex) for row in pageidDF.collect()]

print("{} - Total pages selected after filter Count : {} ".format(datetime.now(),len(pageidlist)))

"""Process the data for the category of interest

"""

data_process_start_time = time.time()
print("{} - started reading the data ".format(datetime.now()))

data = df.filter('redirect IS NULL').selectExpr('id','title',
    'revision.text._VALUE AS text'
).filter('redirect IS NULL').filter('ns == 0')

InfoData = data.filter(col('id').isin(pageidlist)).cache()

# InfoData.select(getInfoboxUDF('text').alias('info')).collect()

"""Extract the Movie entities """

movieEntityUDF = udf(lambda x : extractMovieEntity(x) ,MapType(StringType(),StringType())  )

infodata= InfoData.select('title',getInfoboxUDF(col('text')).alias('info'))

movieEntityNeodf = infodata.select('title',movieEntityUDF('info').alias('MovieEntity')).toPandas()

movieEntityDFlist = list(movieEntityNeodf['MovieEntity'])

movieEntityLoadDF = pd.DataFrame(movieEntityDFlist)

"""#### Extract relation between Movie and person"""

PersonRelationUDF = udf(lambda x : extractPersonRelation(x),ArrayType(StringType()))

relationPersonDF = infodata.select('title',PersonRelationUDF('info').alias('extractPersonRelation')).select('title',explode(col('extractPersonRelation')).alias('PersonRelation'))

movie_person_rel_df = relationPersonDF.select('title',split(col('PersonRelation'),'~')[0].alias('relation'),split(col('PersonRelation'),'~')[1].alias('person')).toPandas()

movie_person_rel_df.tail()

"""Get details associated with Person"""

person_entity = pd.DataFrame(movie_person_rel_df['person'].unique(),columns=['name'])

end_time = time.time()
print("{} - completed the data processing ".format(datetime.now()))

print(end_time-data_start_time)
print(index_start_time - data_start_time)
print(end_time-data_process_start_time)
print( data_process_start_time - data_start_time )

""" Loading Data into Neo 4 j"""

neo_config_file = open('neo4j.config','r')
contents = neo_config_file.read().split('\n')
uri = contents[0].split('=')[1]
pwd = contents[1].split('=')[1]
user= contents[2].split('=')[1]
conn = Neo4jConnection(uri,user,pwd)

"""Clean the existing in graph db"""

delete_all_nodes = 'MATCH (n) DETACH DELETE n;'

conn.query(delete_all_nodes)

"""Load Movie data into the  node in Neo4j"""

query = '''
UNWIND $rows as row
 CREATE (e:Movie {  title : row.name, name : row.name ,budget : row.budget, released :row.released ,runtime :row.runtime } )
 '''
batch_size = 1000
batch_id = 0 
while batch_id < len(movieEntityLoadDF)/batch_size:

  res = conn.query(query, parameters = {'rows':movieEntityLoadDF[batch_id*batch_size: (batch_id+1)*batch_size].reset_index().to_dict('records')})
  batch_id += 1

"""Load the data into the node in Neo4j"""

query = '''
UNWIND $rows as row
 CREATE (e:Person { name : row.name } )
 '''

batch_size = 1000
batch_id = 0 

while batch_id < len(person_entity)/batch_size:

  res = conn.query(query, parameters = {'rows':person_entity[batch_id*batch_size: (batch_id+1)*batch_size].reset_index().to_dict('records')})
  batch_id += 1

"""Load the relationship between Movie and person """

query = ''' 
UNWIND $rows as row
MATCH (entity1:Movie {name: row.title}),(entity2:Person {name: row.person})
CALL apoc.create.relationship(entity1, row.relation,NULL, entity2) YIELD rel
RETURN entity1.name, type(rel), entity2.name 
'''


batch_size = 10
batch_id = 1 

while batch_id < len(movie_person_rel_df)/batch_size:
# while batch_id < 3:

  res = conn.query(query, parameters = {'rows':movie_person_rel_df[batch_id*batch_size: (batch_id+1)*batch_size].reset_index(drop=True).to_dict('records')})
  batch_id += 1

print("{} - completed the loading data to neo4j ".format(datetime.now()))

datetime.now()